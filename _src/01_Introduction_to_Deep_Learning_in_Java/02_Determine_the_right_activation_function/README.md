
Note that there are no implied rules about using an activation function at different layers. It all depends on your data and what you want to do from it. The only purpose of an activation function is to bring non-linearity in the network.

| Constraint | Activation function |
|--|--|
| Hidden layers| ReLU |
| Output layer (classification)  | Sigmoid |
| Output layer (non-classification problems) | Linear
| 


